{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3Qv98XfKpqUUQLbn42UMY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammaddany/Contextual-Image-Caption-Creation-Using-Object-Positional-Embedding-and-Generative-Models/blob/main/Untitled23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload images Dataset\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "# Get the uploaded file name dynamically\n",
        "\n",
        "# Read the image file\n",
        "test_img = cv2.imread(filename)\n",
        "# Using the same variable as before\n",
        "img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Function to plot the image\n",
        "def plot_image(img, cmap=None):\n",
        "plt.imshow(img, cmap=cmap)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "# Display the image\n",
        "plot_image(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YANZvF8fu16L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first create a directory to store the model\n",
        "%mkdir model\n",
        "\n",
        "# enter the directory and download the necessary files\n",
        "%cd model\n",
        "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_optimal/yolov5.weights\n",
        "\n",
        "!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov5.cfg\n",
        "\n",
        "!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/data/coco.names\n",
        "%cd ..\n",
        "\n",
        "# Image Preprocessing: Convert Image to Blob\n",
        "\n",
        "scalefactor = 1.0/255.0\n",
        "new_size = (416, 416)\n",
        "blob = cv2.dnn.blobFromImage(test_img, scalefactor, new_size, swapRB=True, crop=False)\n",
        "\n",
        "# define class labels\n",
        "class_labels_path = \"model/coco.names\"\n",
        "class_labels = open(class_labels_path).read().strip().split(\"\\n\")\n",
        "class_labels\n",
        "\n",
        "# declare repeating bounding box colors for each class\n",
        "# 1st: create a list colors as an RGB string array\n",
        "# Example: Red, Green, Blue, Yellow, Magenda\n",
        "class_colors = [\"255,0,0\",\"0,255,0\",\"0,0,255\",\"255,155,0\",\"255,0, 255\"]\n",
        "\n",
        "#2nd: split the array on comma-separated strings and for change each string type to integer\n",
        "class_colors = [np.array(every_color.split(\",\")).astype(\"int\") for every_color in class_colors]\n",
        "\n",
        "#3rd: convert the array or arrays to a numpy array\n",
        "class_colors = np.array(class_colors)\n",
        "\n",
        "#4th: tile this to get 80 class colors, i.e. as many as the classes(16 rows of 5cols each).\n",
        "# If you want unique colors for each class you may randomize the color generation or set them manually\n",
        "class_colors = np.tile(class_colors,(16,1))\n",
        "\n",
        "def colored(r, g, b, text):\n",
        "return \"\\033[38;2;{};{};{}m{} \\033[38;2;255;255;255m\".format(r, g, b, text)\n",
        "\n",
        "for i in range(16):\n",
        " line = \"\"\n",
        " for j in range(5):\n",
        "    class_id = i*5 + j\n",
        "    class_id_str = str(class_id)\n",
        "    text = \"class\" + class_id_str\n",
        "    colored_text = colored(class_colors[class_id][0], class_colors[class_id][1], class_colors[class_id][2], text)\n",
        "    line += colored_text\n",
        "print(line)\n",
        "\n",
        "# or select the colors randomly\n",
        "class_colors = np.random.randint(0, 255, size=(len(class_labels), 3), dtype=\"uint8\")"
      ],
      "metadata": {
        "id": "dJQUHnBbu18s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "yolo_model = cv2.dnn.readNetFromDarknet('model/   yolov5.cfg','model/yolov5.weights')\n",
        "\n",
        "# Read the network layers/components. The YOLO neural network has 379 components. They consist of convolutional layers (conv), rectifier linear units (relu) etc.:\n",
        "model_layers = yolo_model.getLayerNames()\n",
        "print(\"number of network components: \" + str(len(model_layers)))\n",
        "# print(model_layers)\n",
        "\n",
        "# extract the output layers in the code that follows:\n",
        "# - model_layer[0]: returns the index of each output layer in the range of 1 to 379\n",
        "# - model_layer[0] - 1: corrects  this to the range of 0 to 378\n",
        "# - model_layers[model_layer[0] - 1]: returns the indexed layer name\n",
        "output_layers = [model_layers[model_layer - 1] for model_layer in yolo_model.getUnconnectedOutLayers()]\n",
        "\n",
        "# YOLOv5 deploys the same YOLO head as YOLOv4 for detection with the anchor based detection steps, and three levels of detection granularity.\n",
        "print(output_layers)\n",
        "\n",
        "# input pre-processed blob into the model\n",
        "yolo_model.setInput(blob)\n",
        "\n",
        "# compute the forward pass for the input, storing the results per output layer in a list\n",
        "obj_detections_in_layers = yolo_model.forward(output_layers)\n",
        "\n",
        "# verify the number of sets of detections\n",
        "print(\"number of sets of detections: \" + str(len(obj_detections_in_layers)))\n",
        "def object_detection_analysis(test_image, obj_detections_in_layers, confidence_threshold):\n",
        "\n",
        "# get the image dimensions\n",
        "img_height = test_img.shape[0]\n",
        "img_width = test_img.shape[1]\n",
        "\n",
        "result = test_image.copy()\n",
        "\n",
        "# loop over each output layer\n",
        "for object_detections_in_single_layer in  obj_detections_in_layers:\n",
        "# loop over the detections in each layer\n",
        "  for object_detection in object_detections_in_single_layer:\n",
        "    # obj_detection[1]: bbox center pt_x\n",
        "    # obj_detection[2]: bbox center pt_y\n",
        "    # obj_detection[3]: bbox width\n",
        "    # obj_detection[4]: bbox height\n",
        "    # obj_detection[5]: confidence scores for all detections within the bbox\n",
        "\n",
        "    # get the confidence scores of all objects detected with the bounding box\n",
        "    prediction_scores = object_detection[5:]\n",
        "     # consider the highest score being associated with the winning class\n",
        "    # get the class ID from the index of the highest score\n",
        "    predicted_class_id = np.argmax(prediction_scores)\n",
        "    # get the prediction confidence\n",
        "    prediction_confidence = prediction_scores[predicted_class_id]\n",
        "\n",
        "    # consider object detections with confidence score higher than threshold\n",
        "    if prediction_confidence > confidence_threshold:\n",
        "        # get the predicted label\n",
        "        predicted_class_label = class_labels[predicted_class_id]\n",
        "        # compute the bounding box coordinates scaled for the input image\n",
        "        # scaling is a multiplication of the float coordinate with the appropriate  image dimension\n",
        "        bounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
        "        # get the bounding box centroid (x,y), width and height as integers\n",
        "        (box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
        "        # to get the start x and y coordinates we to subtract from the centroid half the width and half the height respectively\n",
        "        # for even values of width and height of bboxes adjacent to the  image border\n",
        "        #  this may generate a -1 which is prevented by the max() operator below\n",
        "        start_x_pt = max(0, int(box_center_x_pt - (box_width / 2)))\n",
        "        start_y_pt = max(0, int(box_center_y_pt - (box_height / 2)))\n",
        "        end_x_pt = start_x_pt + box_width\n",
        "        end_y_pt = start_y_pt + box_height\n",
        "\n",
        "        box_color = class_colors[predicted_class_id]\n",
        "\n",
        "        # convert the color numpy array as a list and apply to text and box\n",
        "        box_color = [int(c) for c in box_color]\n",
        "\n",
        "        # print the prediction in console\n",
        "        predicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label, prediction_confidence * 100)\n",
        "        print(\"predicted object {}\".format(predicted_class_label))\n",
        "\n",
        "        # draw the rectangle and text in the image\n",
        "        cv2.rectangle(result, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 1)\n",
        "        cv2.putText(result, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 1)\n",
        "return result\n",
        "\n",
        "confidence_threshold = 0.2\n",
        "result_raw = object_detection_analysis(test_img, obj_detections_in_layers, confidence_threshold)\n",
        "\n",
        "result_img = cv2.cvtColor(result_raw, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(result_img)\n",
        "plt.show()\n",
        "\n",
        "class_ids_list = []\n",
        "boxes_list = []\n",
        "confidences_list = []\n",
        "\n",
        "def object_detection_attributes(test_image, obj_detections_in_layers, confidence_threshold):\n",
        "# get the image dimensions\n",
        "img_height = test_img.shape[0]\n",
        "img_width = test_img.shape[1]\n",
        "\n",
        "# loop over each output layer\n",
        "for object_detections_in_single_layer in obj_detections_in_layers:\n",
        "# loop over the detections in each layer\n",
        "for object_detection in object_detections_in_single_layer:\n",
        "  # get the confidence scores of all objects detected with the bounding box\n",
        "  prediction_scores = object_detection[5:]\n",
        "  # consider the highest score being associated with the winning class\n",
        "  # get the class ID from the index of the highest score\n",
        "  predicted_class_id = np.argmax(prediction_scores)\n",
        "  # get the prediction confidence\n",
        "  prediction_confidence = prediction_scores[predicted_class_id]\n",
        "\n",
        "  # consider object detections with confidence score higher than threshold\n",
        "  if prediction_confidence > confidence_threshold:\n",
        "    # get the predicted label\n",
        "    predicted_class_label = class_labels[predicted_class_id]\n",
        "    # compute the bounding box coordinates scaled for the input image\n",
        "    bounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
        "    (box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
        "    start_x_pt = max(0, int(box_center_x_pt - (box_width / 2)))\n",
        "    start_y_pt = max(0, int(box_center_y_pt - (box_height / 2)))\n",
        "\n",
        "    # update the 3 lists for nms processing\n",
        "    # - confidence is needed as a float\n",
        "    # - the bbox info has the openCV Rect format\n",
        "    class_ids_list.append(predicted_class_id)\n",
        "    confidences_list.append(float(prediction_confidence))\n",
        "    boxes_list.append([int(start_x_pt), int(start_y_pt), int(box_width), int(box_height)])\n",
        "score_threshold = 0.5\n",
        "object_detection_attributes(test_img, obj_detections_in_layers, score_threshold)\n",
        "# NMS for a set of overlapping bboxes returns the ID of the one with highest\n",
        "# confidence score while suppressing all others (non maxima)\n",
        "# - score_threshold: a threshold used to filter boxes by score\n",
        "# - nms_threshold: a threshold used in non maximum suppression.\n",
        "\n",
        "score_threshold = 0.5\n",
        "nms_threshold = 0.4\n",
        "winner_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, score_threshold, nms_threshold)\n",
        "\n",
        "# loop through the final set of detections remaining after NMS and draw bounding box and write text\n",
        "for winner_id in winner_ids:\n",
        "max_class_id = winner_id\n",
        "box = boxes_list[max_class_id]\n",
        "start_x_pt = box[0]\n",
        "start_y_pt = box[1]\n",
        "box_width = box[2]\n",
        "box_height = box[3]\n",
        "\n",
        "#get the predicted class id and label\n",
        "predicted_class_id = class_ids_list[max_class_id]\n",
        "predicted_class_label = class_labels[predicted_class_id]\n",
        "prediction_confidence = confidences_list[max_class_id]\n",
        "\n",
        " #obtain the bounding box end coordinates\n",
        "end_x_pt = start_x_pt + box_width\n",
        "end_y_pt = start_y_pt + box_height\n",
        "\n",
        "#get a random mask color from the numpy array of colors\n",
        "box_color = class_colors[predicted_class_id]\n",
        "\n",
        "#convert the color numpy array as a list and apply to text and box\n",
        "box_color = [int(c) for c in box_color]\n",
        "\n",
        "# print the prediction in console\n",
        "predicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label, prediction_confidence * 100)\n",
        "print(\"predicted object {}\".format(predicted_class_label))\n",
        "\n",
        "# draw rectangle and text in the image\n",
        "cv2.rectangle(test_img, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 2)\n",
        "cv2.putText(test_img, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)\n",
        "\n",
        "test_imgz = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(test_imgz)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ntZWS-viu1_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scene Graph Generation\n",
        "# Install dependencies\n",
        "!git clone https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch.git\n",
        "%cd Scene-Graph-Benchmark.pytorch\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py build develop\n",
        "\n",
        "# Load detected objects\n",
        "detected_objects = [\n",
        "{\"label\": \"dog\", \"bbox\": [200, 300, 400, 500]},\n",
        "{\"label\": \"person\", \"bbox\": [50, 100, 250, 400]},\n",
        "{\"label\": \"person\", \"bbox\": [400, 150, 600, 500]}\n",
        "]\n",
        "\n",
        "# Save detections in JSON format (SGG model input)\n",
        "with open(\"detected_objects.json\", \"w\") as f:\n",
        "json.dump(detected_objects, f)\n",
        "\n",
        "print(\"Saved detected objects for scene graph generation!\")\n",
        "\n",
        "!pip install torch torchvision\n",
        "!pip install cython yacs matplotlib tqdm\n",
        "!apt-get install ninja-bui\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch.git\n",
        "%cd Scene-Graph-Benchmark.pytorch\n",
        "\n",
        "# Install required Python packages\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Build the project\n",
        "!python setup.py build develop\n",
        "\n",
        "from scene_graph_benchmark.demo.predictor import SceneGraphPredictor\n",
        "\n",
        "# Load model (pre-trained on Visual Genome dataset)\n",
        "model = SceneGraphPredictor(\"checkpoints/motifnet_sgdet.tar\")\n",
        "\n",
        "# Load detected objects\n",
        "with open(\"detected_objects.json\") as f:\n",
        "detected_objects = json.load(f)\n",
        "\n",
        "# Predict relationships\n",
        "scene_graph = model.predict(detected_objects)\n",
        "\n",
        "# Save scene graph\n",
        "with open(\"scene_graph.json\", \"w\") as f:\n",
        "json.dump(scene_graph, f, indent=4)\n",
        "\n",
        "print(\"Scene Graph Generated & Saved!\")\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load scene graph\n",
        "with open(\"scene_graph.json\") as f:\n",
        "scene_graph = json.load(f)\n",
        "\n",
        "# Create graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes (objects)\n",
        "for obj in scene_graph[\"objects\"]:\n",
        "G.add_node(obj)\n",
        "\n",
        "# Add edges (relationships)\n",
        "for rel in scene_graph[\"relationships\"]:\n",
        "G.add_edge(rel[\"subject\"], rel[\"object\"], label=rel[\"predicate\"])\n",
        "\n",
        "# Draw graph\n",
        "pos = nx.spring_layout(G)\n",
        "plt.figure(figsize=(8, 6))\n",
        "nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"lightblue\", font_size=10, edge_color=\"gray\")\n",
        "edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color=\"red\")\n",
        "plt.title(\"Generated Scene Graph\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VkDZbomJu2B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#caption creation\n",
        "def GPT_Completion(texts) :\n",
        "## Call the API key under your account (in a secure way)\n",
        "response = GPT_Completion(scene_graph_text)\n",
        "print(\"Generated Caption:\", response)"
      ],
      "metadata": {
        "id": "Cg3ZpJaJu2EC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}